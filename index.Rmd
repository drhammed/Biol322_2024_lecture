---
title: "BIOL 322: Biostatistics"
subtitle: "Guest Lecture: Testing Model Assumptions"
author: "Hammed Akande"
institute: "Laboratory of Quantitative and Community Ecology, Concordia University"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```


class: inverse, center, middle

# Testing Hypothesis of Linear Regression

---
# Goals of the lecture

- **Understand the assumptions underlying a regression model.**

- **Understand the implications of violating assumptions.**

- **Evaluate regression model assumptions through visual representations and statistical tests.**

- **Identifying observations within regression models.**


---

# INTRODUCTION


- Recall that our linear regression model can be defined as:

$$ Y = X\beta  +  \epsilon $$

where: $Y$ = response, $X$ = predictor(s), $\beta$ = coefficient(s), $\epsilon$ = error term

--


- We then use that equation to estimate our $\beta$ parameter(s) and noted that the individual estimates $\hat{\beta}_j$ are typically assumed to follow a **normal distribution** with _mean_ $\beta_j$ and _variance_. 
Such that:
$$\hat{\beta}_j \sim N\left(\beta_j, \sigma^2 M_{jj}  \right)$$
where:

**mean** $=$ $\beta_j$, 

**variance** $=$  $\sigma^2 M_{jj}$

**M** $=$ $\left(X^\top X\right)^{-1}$


---
# INTRODUCTION

- Multiple metrics, including $R^2$, and **RMSE** are used to assess the adequacy of our model in capturing the underlying patterns in our data. Each of these metrics, to some extent, revolves around the expression:
$$\sum_{i = 1}^n (y_i - \hat{y}_i)^2$$

where $y_i$ is the observed value and $\hat{y}_i$ is the predicted value.


- So, technically, they gauge the goodness of fit. But, what about the assumptions underlying the model?



--

_**So, do we really care about how well our model fits the data?**_ 

--

- If you want to make inference from the model, then our considerations become crucial. 


---
class: inverse, center, middle

# Assumptions of Linear Regression Models 


---
# Assumptions of linear regression

Basically, model assumptions are explicitly embedded in a model statement,

$$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{n-1} x_{i(n-1)} + \epsilon_i$$

where $\epsilon_i \sim N(0, \sigma^2).$


And the assumptions can be represented as **LINE**:

--
- **L**inearity: the response is a linear combination of the predictors. (With noise about this true linear relationship.)

--
- **I**ndependence: the errors are independent of each other.

--
- **N**ormality: the distribution of the errors typically follow a normal distribution.

--
- **E**quality of Variance: the variance of error is the same at any set of predictor values.


---

# Assumptions of linear regression

- The linearity assumption is encoded as

$$\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i(p-1)}$$

- while the remaining three, are all encoded in

$$\epsilon_i \sim N(0, \sigma^2)$$

the $\epsilon_i$ are $iid$ normal random variables with equal variance.


- If these assumptions are satisfied, then that's **excellent**! We can make inference, and our conclusions will be reliable.

--

- However, if these assumptions are not fulfilled, we can still perform some test (e.g.,  a t-test in R), but the outcomes will lack **validity**. The distributions of parameter estimates will deviate from expectations, leading to erroneous acceptance or rejection of hypotheses.


---
class: inverse, center, middle

# Assessing Model Assumptions

---

# How do we assess our model assumptions?



## 1. Fitted vs Residuals Plot

This is one of the most important metrics in checking model assumptions, especially for linearity and equal variance.


- To demonstrate this method, we can work with (simulated) data from three models:

--

$$\text{Model A:} \quad Y = 2 + 10x + \epsilon, \quad \epsilon \sim N(0, 1)$$

$$ \text{Model B:} \quad Y = 2 + 10x + \epsilon, \quad \epsilon \sim N(0, x^2)$$

$$\text{Model C:} \quad Y = 2 + 10x^2 + \epsilon, \quad \epsilon \sim N(0, 16)$$


--
_**Which of the three models defined above do you think would not violate any assumptions?**_



---
# Define the function for the models

```{r, echo = TRUE}

mod_1 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x + rnorm(n = sample_size, mean = 0, sd = 1)
  data.frame(x, y)
}

mod_2 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x + rnorm(n = sample_size, mean = 0, sd = x)
  data.frame(x, y)
}

mod_3 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 4)
  data.frame(x, y)
}
```


---
# Evaluate model assumptions using plots

- Let's demonstrate Fitted vs residual plot using (simulated) data from the three models:

--

.pull-left[

- Recall that we said model A will show almost no sign of model violation, we can use that to demonstrate what a "good" plot looks like.


```{r, echo = TRUE}
set.seed(99)
data_1 = mod_1()
#head(data_1)

```
]

--

.pull-right[
- Now, let's fit the model and add the fitted line to the scatterplot.
```{r, echo = FALSE, fig.height = 7, fig.width = 6}
plot(y ~ x, data = data_1, col = "grey", pch = 20, main = "Model A")
fit_1 = lm(y ~ x, data = data_1)
abline(fit_1, col = "darkred", lwd = 3)
```
]


---
# Evaluate model assumptions using plots

- Plot a fitted vs residuals plot.  

.pull-left[
```{r, echo = FALSE, fig.height=8}
plot(fitted(fit_1), resid(fit_1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Model A")
abline(h = 0, col = "darkred", lwd = 2)
```
]

--

.pull-right[

Two things here:
        
1. For each fitted value, residuals should be ~ $0$. If this is true, the assumption of *linearity* is met. Hence, why we added a line at $y = 0$.
2. At every fitted value, the spread/dispersion of the residuals should be relatively constant. If so, the assumption of *equal variance* holds true.

Here, we can observe that both conditions are satisfied. The residuals are centered around zero, and the spread of the residuals is consistent across the range of fitted values. 


]


---

# Model B

Now, lets check an instance where  one of the assumptions is violated. We can use **model B** for this purpose. 

```{css echo=FALSE}
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
```

--

```{r, echo = FALSE}
set.seed(99)
data_2 = mod_2()
fit_2 = lm(y ~ x, data = data_2)
plot(y ~ x, data = data_2, col = "grey", pch = 20, main = "Model B")
abline(fit_2, col = "darkred", lwd = 3)
```

--
<div class="side-note">
      <p>
        In this case, the variance is larger for larger values of the predictor variable $x$. This is a clear indication that the assumption of equal variance is violated.
      </p>

</div>  


---

# Model B


```{css echo=FALSE}
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
```



```{r, echo = FALSE}
plot(fitted(fit_2), resid(fit_2), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Model B")
abline(h = 0, col = "darkred", lwd = 2)
```

--

<div class="side-note">
      <p>
        Here, two observations are obvious here. Firstly, the residuals appear to be roughly centered around 0 for any given fitted value, indicating that the linearity assumption holds. However, it's also evident that for higher fitted values, the spread of the residuals increases, indicating a violation of the constant variance assumption. 
      </p>

</div>    



---

```{css echo=FALSE}
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}

.plot-container {
  display: flex;
  justify-content: space-between; /* Add space between the plots */
}

.plot-container img {
  width: 300%; 
}
```


# Model C

.pull-left[
- Next, we will illustrate a model that fails to satisfy the linearity assumption.

- Recall _Model C_: $$\quad Y = 2 + 10x^2 + \epsilon, \hspace1ex \epsilon \sim N(0, 16)$$

- **Model C** serves as an example where $(Y)$ is not a linear combination of the predictors. In this scenario, the predictor is $(x)$, but the model incorporates $(x^2)$.

]

--
.pull-right[
- Plot the fitted vs residual plot for model C.

```{r, echo = FALSE}

set.seed(99)
data_3 = mod_3()
fit_3 = lm(y ~ x, data = data_3)


#par(mfrow=c(1,2)) 
plot(y ~ x, data = data_3, col = "grey", pch = 20, main = "Model C")
abline(fit_3, col = "darkred", lwd = 3)

```
]

---
# Model C

```{css echo=FALSE}
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
```


```{r, echo = FALSE}


plot(fitted(fit_3), resid(fit_3), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Model C")
abline(h = 0, col = "darkorange", lwd = 2)



```


--

<div class="side-note">
      <p>
        Here, we can see that the equal variance assumption holds. However, the linearity assumption is violated. The residuals are not centered around zero for any given fitted value. This is a clear indication that the model is not capturing the true relationship between the predictors and the response.
      </p>

</div>    


---
class: inverse, center, middle

# Formal (statistical) Tests

---

# Statistical Test

Another more formal test that can be used to test the assumption of equality of variance is the **Levene and Breusch-Pagan Test**.  

--

- Constant variance can be referred to as **Homogeneity of Variance or Homoscedasticity**.

- Conversely, non-constant variance is called **Heteroscedasticity or heteogrenity of variance**. 

--

The Levene Test can be performed using the `leveneTest` function from the `car` package. While the Breusch-Pagan Test can be performed using the `bptest` function from the `lmtest` package.

```{r, echo = TRUE}
#install.packages("car", dependecies=TRUE)
library(car)
#install.packages("lmtest")
library(lmtest)
```

--
The null and alternative hypotheses are as follows:


$H_0$: Homoscedasticity. The errors maintain equal variance about the true model.

$H_a$: Heteroscedasticity.  The errors exhibit non-equal variance about the true model.


---
# Fit the model

To demonstrate how to use `Breusch-Pagan` and `Levene` test, we will use the three models we have been working with. 

Remember,

- `fit_1` had no violation of assumptions (both linearity and equal variance were satisfied),
- `fit_2` linearity holds but violated the equal variance assumption,
- `fit_3` equal variance holds, but violated linearity


---

# Breusch-Pagan Test

.pull-left[
- model_1 - `bptest(fit_1)`

```{r, echo = FALSE}
bptest(fit_1)
```
]

--
.pull-right[

- model_2 - `bptest(fit_2)`

```{r, echo = FALSE}
bptest(fit_2)
```
]

--

- model_3 - `bptest(fit_3)`

```{r, echo = FALSE}
bptest(fit_3)
```


---

# Levene Test 


- model_1

```{r, echo = FALSE}

# Create groups based on x
data_1$group <- cut(data_1$x, breaks = 2)

# Apply Levene Test on residuals with grouping
leveneTest(y ~ group, data = data_1)

```



--

- model_2

```{r, echo = FALSE}
# Create groups based on x
data_2$group <- cut(data_2$x, breaks = 2)

# Apply Levene Test on residuals with grouping
leveneTest(y ~ group, data = data_2)


```


--

- model_3

```{r, echo = FALSE}

# Create groups based on x
data_3$group <- cut(data_3$x, breaks = 2)

# Apply Levene Test on residuals with grouping
leveneTest(y ~ group, data = data_3)

```


---
class: inverse, center, middle

# Normality Assumption


---

#Assess Normality assumption

**Histogram** and **QQ plot** are used to assess the normality assumption. The histogram is used to visualize the distribution of the residuals, while the QQ plot is used to compare the distribution of the residuals to the normal distribution.

--

- Histogram

```{r, echo=FALSE, fig.height=5, fig.width=15}
par(mfrow = c(1, 3))
hist(resid(fit_1),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_1",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_2),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_2",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_3),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_3",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
```



--

**_Looking at the above plots, can you tell me which one appears normal or not?_**


---

# Q-Q Plots

We can also use another powerful approach to assess the normality of errors- a normal quantile-quantile plot (**Q-Q plot**)

We can use the `qqnorm()` function in `R` to plot the points, and adds the line to the plot using the `qqline()` function. We create a Q-Q plot for the residuals of `fit_1` to check if the errors could truly be normally distributed.

--

```{r, echo=FALSE, fig.height=5, fig.width=15}
par(mfrow = c(1, 3))
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_2), main = "Normal Q-Q Plot, fit_2", col = "darkgrey")
qqline(resid(fit_2), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_3), main = "Normal Q-Q Plot, fit_3", col = "darkgrey")
qqline(resid(fit_3), col = "dodgerblue", lwd = 2)
```

--

**_Looking at the above plots, can you tell me which one appears normal or not?_**


---

# Shapiro Wilk Test

Shapiro-Wilk test is another test that can be used to test the normality of the residuals. The null hypothesis of the test is that the data is normally distributed.

We can use the `shapiro.test()` function in `R` to perform the test.

--

The null and alternative hypotheses are as follows:

$H_0$: The data is normally distributed.

$H_a$: The data is not normally distributed.


---
# Shapiro Wilk Test

.pull-left[
- Model_1 - `shapiro.test(resid(fit_1))`

```{r, echo=FALSE}
shapiro.test(resid(fit_1))
```

]

--

.pull-right[

- Model_2 - `shapiro.test(resid(fit_2))`


```{r, echo=FALSE}
shapiro.test(resid(fit_2))
```
]

--

- Model_3 - `shapiro.test(resid(fit_3))`


```{r, echo=FALSE}
shapiro.test(resid(fit_3))
```



---
class: inverse, center, middle

# Using real data to assess model assumptions


---

# Example with real data

We can also use real data to do assess model assumption. Fit an additive regression to the `iris` data with `Sepal.Length` as the response and `Petal.Length` and `Species` as predictors

--

```{r, echo=FALSE}

data(iris)
mod_fit <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)
```

.pull-left[
- Check for homogeneity of variance
```{r, echo = FALSE, fig.height=6.5, fig.width=8}
plot(fitted(mod_fit), resid(mod_fit), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residual",
     main = "iris: Fitted vs Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
```
]

--

.pull-right[

```{r, echo=FALSE}
bptest(mod_fit)
```

We can see here that the P value is > 0.05, thus, there is no evidence to reject the $H_0$ (they have equal variance). Put simply, the residuals are **homoscedastic**.
]



---

# Normality Assumption

We will use the qqnorm to check for normality assumption ands verify this using the shapiro test.

.pull-left[

```{r, echo=FALSE, fig.height=8, fig.width=8}
qqnorm(resid(mod_fit), col = "darkgrey")
qqline(resid(mod_fit), col = "dodgerblue", lwd = 2)
```
]

--

.pull-right[


- Confirm with Shapiro Wilk Test

```{r, echo=FALSE}
shapiro.test(resid(mod_fit))
```

From the Shapiro normality test, the P-value > 0.05 (at any conventional level of significance), we have strong evidence to conclude that this data comes from a normal population, as such **normality assumption** is met. 

]





