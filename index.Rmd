---
title: "BIOL 322: Biostatistics"
subtitle: "Guest Lecture: Testing Model Assumptions"
author: "Hammed Akande"
institute: "Laboratory of Quantitative and Community Ecology, Concordia University"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```


class: inverse, center, middle

# Testing Hypothesis of Linear Regression

---
# Goals of the lecture

- **Understand the assumptions underlying a regression model.**

- **Understand the implications of violating assumptions.**

- **Evaluate regression model assumptions through visual representations and statistical tests.**

- **Identifying observations within regression models.**


---


# INTRODUCTION- <span style="font-size: 30px;">What is "Linear"?</span>

- Recall that a linear regression model has an equation of the form:

$$ Y= a + \beta X +  \epsilon  $$

where: $Y$ = response, $X$ = predictor, $\beta$ = slope, a = intercept and $\epsilon$ = error term.


![](images/SLR_pic1.png)


---

# Slope & Intercept

## **Slope <span style="font-size: 20px;">(regression coefficient)</span>**

- The rate at which the response variable changes with respect to the predictor variable.

--

- It is the change in the response variable for a one-unit change in the predictor variable.

--

  - A slope of 0.6 means that for every 1-unit change in X, there will be a 0.6-unit change in Y.

--

<br>

## **Intercept**

--

- The value of the response variable when the predictor variable is zero.






---
#CONTD.

- We then use that equation to estimate our $\beta$ parameter(s) and noted that the individual estimates $\hat{\beta}_j$ are typically assumed to follow a **normal distribution** with _mean_ $\beta_j$ and _variance_. 
Such that:
$$\hat{\beta}_j \sim Normal\left(\beta_j, \sigma^2 M_{jj}  \right)$$
where:

**mean** $=$ $\beta_j$, **variance** $=$  $\sigma^2 M_{jj}$, **M** $=$ $\left(X^\top X\right)^{-1}$



<br style="margin-bottom: 20px;">

--

_**But, what about the assumptions underlying the model?**_




---
class: inverse, center, middle

# Assumptions of Linear Regression Models 


---
# Assumptions of linear regression

Basically, model assumptions are explicitly embedded in a model statement,

$$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{n-1} x_{i(n-1)} + \epsilon_i$$

where $\epsilon_i \sim Normal(0, \sigma^2).$


<br>

--

The assumptions are:

--
- **L**inearity: the response is a linear combination of the predictors. (With noise about this true linear relationship.)

--
- **I**ndependence: the errors are independent of each other.

--
- **E**quality of Variance: the variance of error is the same at any set of predictor values.

--
- **N**ormality: the distribution of the errors typically follow a normal distribution.


--

Which can be shortened as **LIEN**

---

# Assumptions of linear regression

- The **linearity** assumption is encoded in

$$\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i(p-1)}$$

- while the remaining three, are all encoded in

$$\epsilon_i \sim N(0, \sigma^2)$$

the $\epsilon_i$ are $iid$ normal random variables with equal variance.

--

<br style="margin-bottom: 10px;">

- If these assumptions are satisfied, then that's **excellent**! We can make inference, and our conclusions will be reliable.

--

- However, if these assumptions are not fulfilled, we can still perform some test (e.g.,  a t-test in R), but the outcomes will lack **validity**. The distributions of parameter estimates will deviate from expectations, leading to erroneous acceptance or rejection of hypotheses.


---
class: inverse, center, middle

# Assessing Model Assumptions

---

# <span style="font-size: 40px;">How do we assess our model assumptions?</span>



## 1. Fitted vs Residuals Plot

This is one of the most important metrics in checking model assumptions, especially for linearity and equal variance.


To demonstrate this method, we can work with (simulated) data from three models:

--
  - Simulation involves generating (artificial) data that mimic real-world scenarios. 

--

  - To explore the behaviour of the method, assess the performance, test hypotheses, and gain insights into the underlying processes without relying solely on empirical data


---

# Generate the data

$$ \text{Model A:} \quad Y = 2 + 10x + \epsilon, \quad \epsilon \sim N(0, x^2)$$

$$\text{Model B:} \quad Y = 2 + 10x + \epsilon, \quad \epsilon \sim N(0, 1)$$

$$\text{Model C:} \quad Y = 2 + 10x + \epsilon, \quad \epsilon \sim N(4, 4)$$

$$\text{Model D:} \quad Y = 2 + 10x^2 + \epsilon, \quad \epsilon \sim N(0, 16)$$


<br style="margin-bottom: 10px;">

--
_**Which of the three models defined above do you think would not violate any assumptions?**_




```{r, echo = FALSE}
#---
# Define the function for the models

mod_1 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x + rnorm(n = sample_size, mean = 0, sd = x)
  data.frame(x, y)
}

mod_2 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x + rnorm(n = sample_size, mean = 0, sd = 1)
  data.frame(x, y)
}


mod_3 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x + rnorm(n = sample_size, mean = 4, sd = 2)
  data.frame(x, y)
}

mod_4 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 4)
  data.frame(x, y)
}
```


---
# <span style="font-size: 40px;">Fitted vs residual plot using (simulated) data:</span>

Since we said **model B** will show almost no sign of model violation, we'll use that to demonstrate what a "good" plot looks like.


```{r, echo = FALSE}
set.seed(99)
data_1 = mod_2()
#head(data_1)

```

--
.pull-left[
- Add the fitted line to the scatterplot.
```{r, echo = FALSE, fig.height = 7, fig.width = 6}
plot(y ~ x, data = data_1, col = "grey", pch = 20, main = "Model B")
fit_1 = lm(y ~ x, data = data_1)
abline(fit_1, col = "darkred", lwd = 3)
```

]

--
.pull-right[
- Add the fitted vs residuals plot.
```{r, echo = FALSE, fig.height = 7, fig.width = 6}
plot(fitted(fit_1), resid(fit_1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Model B")
abline(h = 0, col = "darkred", lwd = 2)
```
]


---
# Evaluate model assumptions


Two things here:

--

1. For each fitted value, residuals should be ~ $0$. If this is true, the assumption of **linearity** is met. Hence, why we added a line at $y = 0$.

--

2. At every fitted value, the spread/dispersion of the residuals should be relatively constant. If so, the assumption of **equal variance** holds true.

<br>

--

Here, we can observe that both conditions are satisfied. The residuals are centered around zero, and the spread of the residuals is consistent across the range of fitted values. 


---

# <span style="font-size: 30px;">A case of violated assumptions (equal variance)</span>


Now, lets check an instance where  one of the assumptions is violated.
![](images/regression_pic1_resized.png)

--

<div class="side-note">
      <p>
        The standard error of Y|X is the average variability observed around the regression line for any value of X. It is often assumed to be equal across all values of X.
      </p>
      
      This is known as the assumption of homoscedasticity or equal variance.
      
      </p>

</div>  


              


---

# <span style="font-size: 35px;">A case of violated assumptions (equal variance)</span>

We can use **model A** to demonstrate this.
--
.pull-left[

```{r, echo = FALSE,fig.height = 7, fig.width = 6}
set.seed(99)
data_2 = mod_1()
fit_2 = lm(y ~ x, data = data_2)
plot(y ~ x, data = data_2, col = "grey", pch = 20, main = "Model A")
abline(fit_2, col = "darkred", lwd = 3)
```
]

--
.pull-right[

```{r, echo = FALSE,fig.height = 7, fig.width = 6}
plot(fitted(fit_2), resid(fit_2), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Model A")
abline(h = 0, col = "darkred", lwd = 2)
```
]

---

# CONTD.


```{css echo=FALSE}
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
```


Here, two observations are obvious here.

--
- Firstly, the residuals appear to be roughly centered around 0 for any given fitted value, indicating that the **linearity** assumption holds. 

--

- However, it's also evident that for higher fitted values, the spread of the residuals increases, indicating a violation of the **constant variance** assumption.



---

```{css echo=FALSE}
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}

.plot-container {
  display: flex;
  justify-content: space-between; /* Add space between the plots */
}

.plot-container img {
  width: 300%; 
}
```


# <span style="font-size: 35px;">A case of Violated assumptions (linearity)</span>


- Next, we will illustrate a model that fails to satisfy the **linearity** assumption.

--

- Recall _Model D_: $$\quad Y = 2 + 10x^2 + \epsilon, \hspace3ex \epsilon \sim Normal(0, 16)$$


--

- **Model D** serves as an example where $(Y)$ is not a linear combination of the predictor. In this scenario, the predictor is $(x)$, but the model incorporates $(x^2)$.


---

#CONTD.

.pull-left[
- Add the fitted line to the scatterplot.

```{r, echo = FALSE,fig.height = 7, fig.width = 6}

set.seed(99)
data_3 = mod_4()
fit_3 = lm(y ~ x, data = data_3)


#par(mfrow=c(1,2)) 
plot(y ~ x, data = data_3, col = "grey", pch = 20, main = "Model D")
abline(fit_3, col = "darkred", lwd = 3)

```
]


--

.pull-right[
- Plot the fitted vs residuals.

```{r, echo = FALSE,fig.height = 7, fig.width = 6}


plot(fitted(fit_3), resid(fit_3), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Model D")
abline(h = 0, col = "darkorange", lwd = 2)



```

]

---
# CONTD

```{css echo=FALSE}
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
```



- Here, the linearity assumption is violated. The residuals are not centered around zero for any given fitted value. This is a clear indication that the model is not capturing the true relationship between the predictors and the response.


---

# <span style="font-size: 30px;">Residual Plot for Linearity</span>

![](images/linearity_plot_2.png)


---

# <span style="font-size: 30px;">Residual Plot for equal variance</span>

![](images/Homogeneity.png)


---

# <span style="font-size: 30px;">Residual Plot for Independence</span>

![](images/Independence.png)

---
class: inverse, center, middle

# Formal (statistical) Tests

---

# Statistical Test

Another more formal test that can be used to test the assumption of equality of variance is the **Levene Test and Breusch-Pagan Test**.

--

- Equal variance == **Homogeneity of Variance or Homoscedasticity**.

- Unequal variance == **Heteroscedasticity or heterogeneity of variance**. 


--

The Levene Test can be performed using the `leveneTest` function from the `car` package. While the Breusch-Pagan Test can be performed using the `bptest` function from the `lmtest` package.

```{r, echo = FALSE}
#install.packages("car", dependecies=TRUE)
library(car)
#install.packages("lmtest")
library(lmtest)
```

<br>

--
The null and alternative hypotheses are as follows:


$H_0$: Homoscedasticity. The errors maintain equal variance about the true model.

$H_a$: Heteroscedasticity.  The errors exhibit non-equal variance about the true model.


---
# Fit the model

To demonstrate how to use `Breusch-Pagan` test, we will use the three models we have been working with. 

--

Remember,

- `fit_1` had no violation of assumptions (both linearity and equal variance were satisfied),

<br>
--

- `fit_2` linearity holds but violated the equal variance assumption,

<br>
--

- `fit_3` equal variance holds, but violated linearity



---

# Breusch-Pagan Test

The Breusch-Pagan test assesses whether the variance of the residuals in a regression model is constant across different values of the predictor variable.

--

.pull-left[
- model_1 - `bptest(fit_1)`

```{r, echo = FALSE}
bptest(fit_1)
```
]

--
.pull-right[

- model_2 - `bptest(fit_2)`

```{r, echo = FALSE}
bptest(fit_2)
```
]

<br>
--

- model_3 - `bptest(fit_3)`

```{r, echo = FALSE}
bptest(fit_3)
```



---

# Levene Test 

The `Levene` test is designed to assess the equality of variances across different groups.

--

To demonstrate its usefulness, we'll load in a different dataset `Circadian_Data.csv`.

```{r, echo = FALSE}

Circadian <- read.csv("Circadian_Data.csv") # Load the csv named "Circadian"
#order the treatment
Circadian$treatment = ordered(Circadian$treatment, levels = c("control","eyes", "knee"))
# Check your levels now
#levels(Circadian$treatment)
Circadian_anova <- lm( shift ~ treatment, data=Circadian) # Fit the model
#anova(Circadian_anova)
leveneTest(shift ~ factor(treatment), data=Circadian)

```

--

The P value is > 0.05, there is no evidence to reject the $H_0$ (they have the same variance) and the assumption of **homogeneity** is met


<br>

--

*Another example*: **`iris` dataset.** 

```{r, echo = FALSE}

# Load the iris dataset
data(iris)

# Display the first few rows of the dataset
#head(iris)

# Perform the Levene test
levene_test <- leveneTest(Sepal.Length ~ Species, data = iris)
levene_test


```


---
class: inverse, center, middle

# Normality Assumption


---

#Assess Normality assumption

**Histogram** and **QQ plot** are used to assess the normality assumption. The histogram is used to visualize the distribution of the residuals, while the QQ plot is used to compare the distribution of the residuals to the normal distribution.

--

- Histogram

```{r, echo=FALSE, fig.height=5, fig.width=15}
par(mfrow = c(1, 3))
hist(resid(fit_2),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_2",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_1),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_1",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
hist(resid(fit_3),
     xlab   = "Residuals",
     main   = "Histogram of Residuals, fit_3",
     col    = "darkorange",
     border = "dodgerblue",
     breaks = 20)
```



--

**_Looking at the above plots, can you tell me which one appears normal or not?_**


---

# <span style="font-size: 23px;">Q-Q Plots</span>

We can also use a normal quantile-quantile plot (**Q-Q plot**)

--

Let's create a Q-Q plot for the residuals of `fit_1`.

--

```{r, echo=FALSE, fig.height=5.5, fig.width=14}
#We can use the `qqnorm()` function in `R` to plot the points, and adds the line to the plot using the #`qqline()` function. 

#par(mfrow = c(1, 3))
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)
```

--

- If the data points are "close to the line", then the data is *normally* distributed.
- y-axis = observed (sample quantiles), and x-axis = expected values (theoretical quantiles).



---

# <span style="font-size: 25px;">More on Q-Q Plots</span> <span style="font-size: 20px;">(Normal Q-Q Plot from normal & Exponential distribution)</span>

Simulate data from normal distribution to see how "good" Q-Q plot looks like.


```{r, echo=FALSE, fig.height=7.5, fig.width=15}

par(mfrow = c(1, 3))
qqnorm(rnorm(200), main = "Normal Q-Q Plot, Normal Distribution", col = "darkgrey")
qqline(rnorm(200), col = "dodgerblue", lwd = 2)

qqnorm(rnorm(300), main = "Normal Q-Q Plot, Normal Distribution", col = "darkgrey")
qqline(rnorm(300), col = "dodgerblue", lwd = 2)

qqnorm(rnorm(400), main = "Normal Q-Q Plot, Normal Distribution", col = "darkgrey")
qqline(rnorm(400), col = "dodgerblue", lwd = 2)




```


---

# <span style="font-size: 30px;">QQ Plot from exponential distribution</span> 

Now, let's simulate data from an exponential distribution and see how the Q-Q plot looks like.

```{r, echo=FALSE, fig.height=8, fig.width=15}

par(mfrow = c(1, 3))
qqnorm(rexp(200), main = "Normal Q-Q Plot, Exponential Distribution", col = "darkgrey")
qqline(rexp(200), col = "dodgerblue", lwd = 2)

qqnorm(rexp(300), main = "Normal Q-Q Plot, Exponential Distribution", col = "darkgrey")
qqline(rexp(300), col = "dodgerblue", lwd = 2)

qqnorm(rexp(400), main = "Normal Q-Q Plot, Exponential Distribution", col = "darkgrey")
qqline(rexp(400), col = "dodgerblue", lwd = 2)


```


---

# <span style="font-size: 40px;">Back to our (simulated) data</span>

Recall that `fit_1` did not violate any assumption, `fit_2` violated the homogeneity of variance assumption, and `fit_3` violated the linearity assumption.

--

```{r, echo=FALSE, fig.height=7, fig.width=15}
#So, going back to our simulated data, let's create and compare Q-Q plots for `fit_1`, `fit_2`, and `fit_3`.

par(mfrow = c(1, 3))
qqnorm(resid(fit_2), main = "Normal Q-Q Plot, fit_2", col = "darkgrey")
qqline(resid(fit_2), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)

qqnorm(resid(fit_3), main = "Normal Q-Q Plot, fit_3", col = "darkgrey")
qqline(resid(fit_3), col = "dodgerblue", lwd = 2)


```

--
Looking at the above plots, can you tell me which one appears normal or not?


---

# Shapiro Wilk Test

Shapiro-Wilk test is another test that can be used to test the normality of the residuals. The null hypothesis of the test is that the data is normally distributed.

We can use the `shapiro.test()` function in `R` to perform the test.

<br>

--

The null and alternative hypotheses are as follows:

$H_0$: The data is normally distributed.

$H_a$: The data is not normally distributed.


---
# Shapiro Wilk Test

.pull-left[
- Model_1 - `shapiro.test(resid(fit_1))`

```{r, echo=FALSE}
shapiro.test(resid(fit_1))
```

]

--

.pull-right[

- Model_2 - `shapiro.test(resid(fit_2))`


```{r, echo=FALSE}
shapiro.test(resid(fit_2))
```
]

--

- Model_3 - `shapiro.test(resid(fit_3))`


```{r, echo=FALSE}
shapiro.test(resid(fit_3))
```



---
class: inverse, center, middle

# Using real data to assess model assumptions


---

# Example with real data

We can also use real data to do assess model assumption. Fit an additive regression to the `iris` data with `Sepal.Length` as the response and `Petal.Length` and `Species` as predictors

--

```{r, echo=FALSE}

data(iris)
mod_fit <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)
```


.pull-left[
- Check for homogeneity of variance
```{r, echo = FALSE, fig.height = 6.5, fig.width = 8}

plot(fitted(mod_fit), resid(mod_fit), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residual",
     main = "iris: Fitted vs Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

```

]


--

.pull-right[

```{r, echo=FALSE,fig.height = 6.5, fig.width = 8}

bptest(mod_fit)


```


]

--

Here, the P value is > 0.05, thus, there is no evidence to reject the $H_0$ (equal variance assumption is met). Put simply, the residuals are **homoscedastic**.



---

# Normality Assumption

We will use the qqnorm to check for normality assumption ands verify this using the shapiro test.

.pull-left[

```{r, echo=FALSE, fig.height=6.5, fig.width=8}
qqnorm(resid(mod_fit), col = "darkgrey")
qqline(resid(mod_fit), col = "dodgerblue", lwd = 2)
```
]

--

.pull-right[


- Confirm with Shapiro Wilk Test

```{r, echo=FALSE,fig.height = 6.5, fig.width = 8}
shapiro.test(resid(mod_fit))
```

]

--

From the Shapiro normality test, the P-value > 0.05 (at any conventional level of significance), we have strong evidence to conclude that this data comes from a normal population, as such **normality assumption** is met. 



