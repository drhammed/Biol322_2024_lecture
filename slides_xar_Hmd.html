<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>BIOL 322: Biostatistics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Hammed Akande" />
    <meta name="date" content="2024-02-18" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# BIOL 322: Biostatistics
]
.subtitle[
## Guest Lecture: Testing Model Assumptions
]
.author[
### Hammed Akande
]
.institute[
### Laboratory of Quantitative and Community Ecology, Concordia University
]
.date[
### 2024-02-18
]

---





class: inverse, center, middle

# Testing Hypothesis of Linear Regression

---
# Goals of the lecture

- **Understand the assumptions underlying a regression model.**

- **Understand the implications of violating assumptions and what to do when assumptions are violated.**

- **Evaluate regression model assumptions through visual representations and statistical tests.**

- **Understand the concepts of outliers, and influential points.**

- **Identifying observations within regression models.**


---

# INTRODUCTION


- Recall that our linear regression model can be defined as:

$$ Y = X\beta  +  \epsilon $$

where: `\(Y\)` = response, `\(X\)` = predictor(s), `\(\beta\)` = coefficient(s), `\(\epsilon\)` = error term

--


- We then use that equation to estimate our `\(\beta\)` parameter(s) and noted that the,the individual estimates `\(\hat{\beta}_j\)` are typically assumed to follow a **normal distribution** with _mean_ `\(\beta_j\)` and _variance_. 
Such that:
`$$\hat{\beta}_j \sim N\left(\beta_j, \sigma^2 M_{jj}  \right)$$`
where:

**mean** `\(=\)` `\(\beta_j\)`, 

**variance** `\(=\)`  `\(\sigma^2 M_{jj}\)`

**M** `\(=\)` `\(\left(X^\top X\right)^{-1}\)`


---
# INTRODUCTION

- Multiple metrics, including `\(R^2\)`, and **RMSE** are used to assess the adequacy of our model in capturing the underlying patterns in our data. Each of these metrics, to some extent, revolves around the expression:
`$$\sum_{i = 1}^n (y_i - \hat{y}_i)^2$$`

where `\(y_i\)` is the observed value and `\(\hat{y}_i\)` is the predicted value.


- So, technically, they gauge the goodness of fit. But, what about the assumptions underlying the model?



--

_**So, do we really care about how well our model fits the data?**_ 

--

- If you want to make inference from the model, then our considerations become crucial. 


---
class: inverse, center, middle

# Assumptions of Linear Regression Models 


---
# Assumptions of linear regression

Basically, model assumptions are explicitly embedded in a model statement,

`$$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i(p-1)} + \epsilon_i$$`

where `\(\epsilon_i \sim N(0, \sigma^2).\)`


And the assumptions can be represented as **LINE**:

--
- **L**inearity: the response is a linear combination of the predictors. (With noise about this true linear relationship.)

--
- **I**ndependence: the errors are independent of each other.

--
- **N**ormality: the distribution of the errors typically follow a normal distribution.

--
- **E**quality of Variance: the variance of error is the same at any set of predictor values.


---

# Assumptions of linear regression

- The linearity assumption is encoded as

`$$\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{i(p-1)}$$`

- while the remaining three, are all encoded in

`$$\epsilon_i \sim N(0, \sigma^2)$$`

the `\(\epsilon_i\)` are `\(iid\)` normal random variables with equal variance.


- If these assumptions are satisfied, then that's **excellent**! We can make inference, and our conclusions will be reliable.

--

- However, if these assumptions are not fulfilled, we can still perform some test (e.g.,  a t-test in R), but the outcomes will lack **validity**. The distributions of parameter estimates will deviate from expectations, leading to erroneous acceptance or rejection of hypotheses.

---

# How do we assess our model assumptions?



## 1. Fitted vs Residuals Plot

This is one of the most important metrics in checking model assumptions, especially for linearity and equal variance.


- To demonstrate this method, we can work with (simulated) data from three models:

--

`$$\text{Model A:} \quad Y = 2 + 10x + \epsilon, \quad \epsilon \sim N(0, 1)$$`

$$ \text{Model B:} \quad Y = 2 + 10x + \epsilon, \quad \epsilon \sim N(0, x^2)$$

`$$\text{Model C:} \quad Y = 2 + 10x^2 + \epsilon, \quad \epsilon \sim N(0, 16)$$`


--
_**Which of the three models defined above do you think would not violate the assumptions?**_



---
# Define the function for the models


```r
mod_1 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x + rnorm(n = sample_size, mean = 0, sd = 1)
  data.frame(x, y)
}

mod_2 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x + rnorm(n = sample_size, mean = 0, sd = x)
  data.frame(x, y)
}

mod_3 = function(sample_size = 1000) {
  x = runif(n = sample_size) * 10
  y = 2 + 10 * x ^ 2 + rnorm(n = sample_size, mean = 0, sd = 4)
  data.frame(x, y)
}
```


---
# Evaluate model assumptions using plots

- Let's demonstrate Fitted vs residual plot using (simulated) data from the three models:

Recall that we said model A will show almost no sign of model violation, we can use that to demonstrate what a "good" plot looks like.



```r
set.seed(99)
data_1 = mod_1()
head(data_1)
```

```
##          x         y
## 1 5.847119  60.70746
## 2 1.137817  14.50560
## 3 6.842647  71.96333
## 4 9.925088 101.48065
## 5 5.349936  53.46300
## 6 9.666141  97.38916
```

---
# Evaluate model assumptions using plots

- Now, let's fit the model and add the fitted line to the scatterplot.
![](slides_xar_Hmd_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---
# Evaluate model assumptions using plots

- Plot a fitted vs residuals plot.  



![](slides_xar_Hmd_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;



---

# Evaluate model assumptions using plots

Two things here:
        
        1. For each fitted value, the average of the residuals should be approximately zero. This validates the assumption of *linearity*. To highlight this, it's common practice to include a horizontal line at `\(y = 0\)`.
        2. At every fitted value, the spread/dispersion of the residuals should be relatively consistent. If so, the assumption of *equal variance* holds true.

--
In this instance, we can observe that both conditions are satisfied. The residuals are centered around zero, and the spread of the residuals is consistent across the range of fitted values. 

--

This suggests that the assumptions of linearity and equal variance are **not violated**.


---

# Model B

Now, lets check an instance where  one of the assumptions is violated. We can use *model B** for this purpose. 

&lt;style type="text/css"&gt;
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
&lt;/style&gt;


![](slides_xar_Hmd_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

--
&lt;div class="side-note"&gt;
      &lt;p&gt;
        In this case, the variance is larger for larger values of the predictor variable `\(x\)`. This is a clear indication that the assumption of equal variance is violated.
      &lt;/p&gt;

&lt;/div&gt;  


---

# Model B


&lt;style type="text/css"&gt;
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
&lt;/style&gt;



![](slides_xar_Hmd_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

--

&lt;div class="side-note"&gt;
      &lt;p&gt;
        Here, two observations are obvious here. Firstly, the residuals appear to be roughly centered around 0 for any given fitted value, indicating that the linearity assumption holds. However, it's also evident that for higher fitted values, the spread of the residuals increases, indicating a violation of the constant variance assumption. 
      &lt;/p&gt;

&lt;/div&gt;    



---

&lt;style type="text/css"&gt;
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}

.plot-container {
  display: flex;
  justify-content: space-between; /* Add space between the plots */
}

.plot-container img {
  width: 300%; 
}
&lt;/style&gt;


# Model C

- Next, we will illustrate a model that fails to satisfy the linearity assumption. **Model C** serves as an example where `\((Y)\)` is not a linear combination of the predictors. In this scenario, the predictor is `\((x)\)`, but the model incorporates `\((x^2)\)`.

--
- plot the fitted vs residual plot for model C.

![](slides_xar_Hmd_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---
# Model C

&lt;style type="text/css"&gt;
.side-note {
  position: absolute;
  top: 50%;
  right: 10px;
  transform: translateY(-50%);
  width: 300px; 
  background-color: #f0f0f0;
  padding: 10px;
  border: 1px solid #ccc;
}
&lt;/style&gt;


![](slides_xar_Hmd_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;


--

&lt;div class="side-note"&gt;
      &lt;p&gt;
        Here, we can see that the equal variance assumption holds. However, the linearity assumption is violated. The residuals are not centered around zero for any given fitted value. This is a clear indication that the model is not capturing the true relationship between the predictors and the response.
      &lt;/p&gt;

&lt;/div&gt;    



---

# Statistical Test

Another more formal test that can be used to test the assumption of equality of variance is the **Levene and Breusch-Pagan Test**.  

--

- Constant variance can be referred to as **Homogeneity of Variance or Homoscedasticity**.

- Conversely, non-constant variance is called **Heteroscedasticity or heteogrenity of variance**. 

--

The Levene Test can be performed using the `leveneTest` function from the `car` package. While the Breusch-Pagan Test can be performed using the `bptest` function from the `lmtest` package.


```r
#install.packages("car", dependecies=TRUE)
library(car)
#install.packages("lmtest")
library(lmtest)
```

--
The null and alternative hypotheses are as follows:


`\(H_0\)`: Homoscedasticity. The errors maintain equal variance about the true model.

`\(H_a\)`: Heteroscedasticity.  The errors exhibit non-equal variance about the true model.


---
# Fit the model

To demonstrate how to use `Breusch-Pagan` and `Levene` test, we will use the three models we have been working with. 

Remember,

- `fit_1` had no violation of assumptions (both linearity and equal variance were satisfied),
- `fit_2` linearity holds but violated the equal variance assumption,
- `fit_3` equal variance holds, but violated linearity


---

# Breusch-Pagan Test

- model_1 - `bptest(fit_1)`


```
## 
## 	studentized Breusch-Pagan test
## 
## data:  fit_1
## BP = 0.28014, df = 1, p-value = 0.5966
```

--

- model_2 - `bptest(fit_2)`


```
## 
## 	studentized Breusch-Pagan test
## 
## data:  fit_2
## BP = 186.2, df = 1, p-value &lt; 2.2e-16
```

--

- model_3 - `bptest(fit_3)`


```
## 
## 	studentized Breusch-Pagan test
## 
## data:  fit_3
## BP = 0.68741, df = 1, p-value = 0.407
```


---

# Levene Test 

- model_1


```
## Levene's Test for Homogeneity of Variance (center = median)
##        Df F value Pr(&gt;F)
## group   1  0.0889 0.7657
##       998
```

--

- model_2


```
## Levene's Test for Homogeneity of Variance (center = median)
##        Df F value  Pr(&gt;F)  
## group   1  3.8933 0.04875 *
##       998                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


--

- model_3


```
## Levene's Test for Homogeneity of Variance (center = median)
##        Df F value    Pr(&gt;F)    
## group   1  558.67 &lt; 2.2e-16 ***
##       998                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---

#Assess Normality assumption

Histogram and QQ plot are used to assess the normality assumption. The histogram is used to visualize the distribution of the residuals, while the QQ plot is used to compare the distribution of the residuals to the normal distribution.

--

- Histogram

![](slides_xar_Hmd_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;



--

**_Looking at the above plots, can you tell me which one appears normal or not?_**


---

# Q-Q Plots

We can also use another powerful approach to assess the normality of errors- a normal quantile-quantile plot (**Q-Q plot**)

We can use the `qqnorm()` function in `R` to plot the points, and adds the line to the plot using the `qqline()` function. We create a Q-Q plot for the residuals of `fit_1` to check if the errors could truly be normally distributed.

--

![](slides_xar_Hmd_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;

--

**_Looking at the above plots, can you tell me which one appears normal or not?_**


---

# Shapiro Wilk Test

Shapiro-Wilk test is another test that can be used to test the normality of the residuals. The null hypothesis of the test is that the data is normally distributed.

We can use the `shapiro.test()` function in `R` to perform the test.

--

The null and alternative hypotheses are as follows:

`\(H_0\)`: The data is normally distributed.

`\(H_a\)`: The data is not normally distributed.


---
# Shapiro Wilk Test

- Model_1 - `shapiro.test(resid(fit_1))`


```
## 
## 	Shapiro-Wilk normality test
## 
## data:  resid(fit_1)
## W = 0.99928, p-value = 0.9738
```


--

- Model_2 - `shapiro.test(resid(fit_2))`



```
## 
## 	Shapiro-Wilk normality test
## 
## data:  resid(fit_2)
## W = 0.96299, p-value = 3.058e-15
```

--

- Model_3 - `shapiro.test(resid(fit_3))`



```
## 
## 	Shapiro-Wilk normality test
## 
## data:  resid(fit_3)
## W = 0.91364, p-value &lt; 2.2e-16
```


---

# Example with real data

We can also use real data to do assess model assumption. Fit an additive regression to the `iris` data with `Sepal.Length` as the response and `Petal.Width` and `Species` as predictors

--



- Check for homogeneity of variance
![](slides_xar_Hmd_files/figure-html/unnamed-chunk-26-1.png)&lt;!-- --&gt;

---

# Check all assumptions


```
## 
## 	studentized Breusch-Pagan test
## 
## data:  mod_fit
## BP = 0.26189, df = 3, p-value = 0.967
```


We can see here that the P value is &gt; 0.05, thus, there is no evidence to reject the `\(H_0\)` (they have equal variance). Put simply, the residuals are **homoscedastic**.


--

![](slides_xar_Hmd_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;

---

# Model Assumptions 

- Confirm with Shapiro Wilk Test


```
## 
## 	Shapiro-Wilk normality test
## 
## data:  resid(mod_fit)
## W = 0.99489, p-value = 0.8805
```

From the Shapiro normality test, the P-value &gt; 0.05 (at any conventional level of significance), we have strong evidence to conclude that this data comes from a normal population, as such **normality assumption** is met. 



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
